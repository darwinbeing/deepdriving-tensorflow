# Evaluation of Network:
* Apply sequence of 34 layers with name "Network":
  *** Layer: Conv_1 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 11x11
        * Stride 4x4
        * Padding VALID
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * with Output-Shape (128, 50, 68, 96) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
      * Padding: SAME
      * Output-Shape: (128, 25, 34, 96)
  *** Layer: Conv_2 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 5x5
        * Stride 1x1
        * Padding SAME
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * Groups 2
          * Group[0]: Input (128, 25, 34, 48), Kernel (5, 5, 48, 128)
          * Group[1]: Input (128, 25, 34, 48), Kernel (5, 5, 48, 128)
        * with Output-Shape (128, 25, 34, 256) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
      * Padding: SAME
      * Output-Shape: (128, 13, 17, 256)
  *** Layer: Conv_3 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * with Output-Shape (128, 13, 17, 384) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
  *** Layer: Conv_4 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * Groups 2
          * Group[0]: Input (128, 13, 17, 192), Kernel (3, 3, 192, 192)
          * Group[1]: Input (128, 13, 17, 192), Kernel (3, 3, 192, 192)
        * with Output-Shape (128, 13, 17, 384) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
  *** Layer: Conv_5 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * Groups 2
          * Group[0]: Input (128, 13, 17, 192), Kernel (3, 3, 192, 128)
          * Group[1]: Input (128, 13, 17, 192), Kernel (3, 3, 192, 128)
        * with Output-Shape (128, 13, 17, 256) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
      * Padding: VALID
      * Output-Shape: (128, 6, 8, 256)
  *** Layer: Dense_6 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * Reshape layer input (128, 6, 8, 256) to vector with 12288 elements.
        * with 4096 Output-Nodes without Bias
        * Weight-Initializer: NormalInitializer(stddev = 0.005)
        * Output-Shape: (128, 4096)
      * Batch-Normalization
      * ReLU Activation function
    * Dropout with keep ratio 0.5
  *** Layer: Dense_7 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * with 4096 Output-Nodes without Bias
        * Weight-Initializer: NormalInitializer(stddev = 0.005)
        * Output-Shape: (128, 4096)
      * Batch-Normalization
      * ReLU Activation function
    * Dropout with keep ratio 0.5
  *** Layer: Dense_8 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * with 256 Output-Nodes without Bias
        * Weight-Initializer: NormalInitializer(stddev = 0.01)
        * Output-Shape: (128, 256)
      * Batch-Normalization
      * ReLU Activation function
    * Dropout with keep ratio 0.5
  *** Layer: Output_9 ***
    * Apply layer "Dense"
      * with 14 Output-Nodes
      * Weight-Decay: 0.0
      * Weight-Initializer: NormalInitializer(stddev = 0.01)
      * Bias-Decay: 0.0
      * Bias-Initializer: ConstantInitializer(0.0)
      * Output-Shape: (128, 14)
    * Sigmoid Activation function
* Output 0 has shape (128, 1)
* Output 1 has shape (128, 1)
* Output 2 has shape (128, 1)
* Output 3 has shape (128, 1)
* Output 4 has shape (128, 1)
* Output 5 has shape (128, 1)
* Output 6 has shape (128, 1)
* Output 7 has shape (128, 1)
* Output 8 has shape (128, 1)
* Output 9 has shape (128, 1)
* Output 10 has shape (128, 1)
* Output 11 has shape (128, 1)
* Output 12 has shape (128, 1)
* Output 13 has shape (128, 1)
Finished to build network with 70533038 trainable variables in 42 tensors.

# Using Checkpoint:
* Checkpoint\State_0\model_880.ckpt

# Using Training-Settings:
{
  "Data": {
    "BatchSize": 96,
    "ImageHeight": 210,
    "ImageWidth": 280,
    "TrainingPath": "F:/Data/DeepDriving/training",
    "ValidatingPath": "F:/Data/DeepDriving/validation"
  },
  "Optimizer": {
    "EpochsPerDecay": 300,
    "LearnRateDecay": 0.5,
    "Momentum": 0.9,
    "Noise": null,
    "StartingLearningRate": 0.01,
    "WeightDecay": 0.0005
  },
  "PreProcessing": {
    "MeanFile": "image-mean.tfrecord"
  },
  "Runner": {
    "Memory": null
  },
  "Trainer": {
    "CheckpointEpochs": 10,
    "CheckpointPath": "Checkpoint",
    "EpochSize": 10000,
    "NumberOfEpochs": 2000,
    "SummaryPath": "Summary"
  },
  "Validation": {
    "Samples": 1000
  }
}

# Using Evaluation-Settings:
{
  "Data": {
    "BatchSize": 128,
    "ImageHeight": 210,
    "ImageWidth": 280,
    "ValidatingPath": "F:/Data/DeepDriving/validation"
  },
  "Evaluator": {
    "CheckpointPath": "Checkpoint",
    "EpochSize": 10000,
    "NumberOfEpochs": 4
  },
  "PreProcessing": {
    "MeanFile": "image-mean.tfrecord"
  },
  "Runner": {
    "Memory": 0.6
  }
}

# Evaluation Results:
Full Summary:  ( Error: 22.74,  SD: 18.23 )

|  Type  | Angle  |   LL   |   ML   |   MR   |   RR   | DistLL | DistMM | DistRR |   L    |   M    |   R    | DistL  | DistR  |  Fast  |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|  MAE   |   0.04 |   0.23 |   0.19 |   0.19 |   0.21 |   4.52 |   4.42 |   5.24 |   0.22 |   0.37 |   0.24 |   3.18 |   3.40 |   0.29 |
|   SD   |   0.07 |   0.37 |   0.41 |   0.44 |   0.42 |   7.29 |   5.74 |   8.04 |   0.46 |   0.82 |   0.54 |   5.93 |   6.36 |   0.42 |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|MAE/Ref | 113.2% | 119.9% | 125.1% | 118.7% | 116.7% |  88.8% |  93.3% |  65.6% |  69.8% | 121.7% |  81.3% |  35.6% |  31.3% |2886.1% |
| SD/Ref |  86.9% |  67.8% |  98.4% | 100.2% |  80.1% |  80.0% |  73.4% |  64.0% |  65.9% | 113.8% |  83.6% |  45.9% |  43.4% | 519.1% |
