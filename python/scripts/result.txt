# Evaluation of Network:
* Apply sequence of 34 layers with name "Network":
  *** Layer: Conv_1 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 11x11
        * Stride 4x4
        * Padding VALID
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * with Output-Shape (128, 50, 68, 96) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
      * Padding: SAME
      * Output-Shape: (128, 25, 34, 96)
  *** Layer: Conv_2 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 5x5
        * Stride 1x1
        * Padding SAME
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * Groups 2
          * Group[0]: Input (128, 25, 34, 48), Kernel (5, 5, 48, 128)
          * Group[1]: Input (128, 25, 34, 48), Kernel (5, 5, 48, 128)
        * with Output-Shape (128, 25, 34, 256) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
      * Padding: SAME
      * Output-Shape: (128, 13, 17, 256)
  *** Layer: Conv_3 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * with Output-Shape (128, 13, 17, 384) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
  *** Layer: Conv_4 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * Groups 2
          * Group[0]: Input (128, 13, 17, 192), Kernel (3, 3, 192, 192)
          * Group[1]: Input (128, 13, 17, 192), Kernel (3, 3, 192, 192)
        * with Output-Shape (128, 13, 17, 384) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
  *** Layer: Conv_5 ***
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * Kernel-Initializer: NormalInitializer(stddev = 0.01)
        * Groups 2
          * Group[0]: Input (128, 13, 17, 192), Kernel (3, 3, 192, 128)
          * Group[1]: Input (128, 13, 17, 192), Kernel (3, 3, 192, 128)
        * with Output-Shape (128, 13, 17, 256) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
      * Padding: VALID
      * Output-Shape: (128, 6, 8, 256)
  *** Layer: Dense_6 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * Reshape layer input (128, 6, 8, 256) to vector with 12288 elements.
        * with 4096 Output-Nodes without Bias
        * Weight-Initializer: NormalInitializer(stddev = 0.005)
        * Output-Shape: (128, 4096)
      * Batch-Normalization
      * ReLU Activation function
    * Dropout with keep ratio 0.5
  *** Layer: Dense_7 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * with 4096 Output-Nodes without Bias
        * Weight-Initializer: NormalInitializer(stddev = 0.005)
        * Output-Shape: (128, 4096)
      * Batch-Normalization
      * ReLU Activation function
    * Dropout with keep ratio 0.5
  *** Layer: Dense_8 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * with 256 Output-Nodes without Bias
        * Weight-Initializer: NormalInitializer(stddev = 0.01)
        * Output-Shape: (128, 256)
      * Batch-Normalization
      * ReLU Activation function
    * Dropout with keep ratio 0.5
  *** Layer: Output_9 ***
    * Apply layer "Dense"
      * with 14 Output-Nodes
      * Weight-Decay: 0.0
      * Weight-Initializer: NormalInitializer(stddev = 0.01)
      * Bias-Decay: 0.0
      * Bias-Initializer: ConstantInitializer(0.0)
      * Output-Shape: (128, 14)
    * Sigmoid Activation function
* Output 0 has shape (128, 1)
* Output 1 has shape (128, 1)
* Output 2 has shape (128, 1)
* Output 3 has shape (128, 1)
* Output 4 has shape (128, 1)
* Output 5 has shape (128, 1)
* Output 6 has shape (128, 1)
* Output 7 has shape (128, 1)
* Output 8 has shape (128, 1)
* Output 9 has shape (128, 1)
* Output 10 has shape (128, 1)
* Output 11 has shape (128, 1)
* Output 12 has shape (128, 1)
* Output 13 has shape (128, 1)
Finished to build network with 70533038 trainable variables in 42 tensors.

# Using Checkpoint:
* Checkpoint\State_0\model_2000.ckpt

# Using Training-Settings:
{
  "Data": {
    "BatchSize": 96,
    "ImageHeight": 210,
    "ImageWidth": 280,
    "TrainingPath": "F:/Data/DeepDriving/training",
    "ValidatingPath": "F:/Data/DeepDriving/validation"
  },
  "Optimizer": {
    "EpochsPerDecay": 300,
    "LearnRateDecay": 0.5,
    "Momentum": 0.9,
    "Noise": null,
    "StartingLearningRate": 0.01,
    "WeightDecay": 0.0005
  },
  "PreProcessing": {
    "MeanFile": "image-mean.tfrecord"
  },
  "Runner": {
    "Memory": null
  },
  "Trainer": {
    "CheckpointEpochs": 10,
    "CheckpointPath": "Checkpoint",
    "EpochSize": 10000,
    "NumberOfEpochs": 2000,
    "SummaryPath": "Summary"
  },
  "Validation": {
    "Samples": 1000
  }
}

# Using Evaluation-Settings:
{
  "Data": {
    "BatchSize": 128,
    "ImageHeight": 210,
    "ImageWidth": 280,
    "ValidatingPath": "F:/Data/DeepDriving/validation"
  },
  "Evaluator": {
    "CheckpointPath": "Checkpoint",
    "EpochSize": 10000,
    "NumberOfEpochs": 4
  },
  "PreProcessing": {
    "MeanFile": "image-mean.tfrecord"
  },
  "Runner": {
    "Memory": 0.6
  }
}

# Evaluation Results:
Full Summary:  ( Error: 16.41,  SD: 16.67 )

|  Type  | Angle  |   LL   |   ML   |   MR   |   RR   | DistLL | DistMM | DistRR |   L    |   M    |   R    | DistL  | DistR  |  Fast  |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|  MAE   |   0.03 |   0.16 |   0.14 |   0.13 |   0.16 |   3.27 |   3.04 |   3.62 |   0.16 |   0.28 |   0.19 |   2.47 |   2.47 |   0.29 |
|   SD   |   0.08 |   0.36 |   0.39 |   0.40 |   0.36 |   5.91 |   4.82 |   7.10 |   0.48 |   0.82 |   0.52 |   5.81 |   5.91 |   0.41 |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|MAE/Ref |  95.8% |  82.7% |  87.8% |  84.6% |  88.9% |  64.3% |  64.1% |  45.4% |  52.2% |  90.3% |  64.3% |  27.8% |  22.7% |2865.7% |
| SD/Ref |  92.9% |  66.7% |  93.3% |  89.3% |  67.5% |  65.0% |  61.6% |  56.5% |  67.7% | 114.5% |  80.7% |  45.0% |  40.4% | 512.6% |
