# Evaluation of Network:
Creating network Graph...
* network Input-Shape: (64, 28, 28, 3)
* Apply sequence of 28 layers with name "Network":
  *** Layer: Conv_1 ***
    * Apply layer "Conv2D"
      * Kernel 3x3
      * Stride 1x1
      * Padding SAME
      * with Output-Shape (64, 28, 28, 128)
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * with Output-Shape (64, 28, 28, 128) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
  *** Layer: Conv_2 ***
    * Apply layer "Conv2D"
      * Kernel 3x3
      * Stride 1x1
      * Padding SAME
      * with Output-Shape (64, 14, 14, 128)
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * with Output-Shape (64, 14, 14, 128) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
  *** Layer: Conv_3 ***
    * Apply layer "Conv2D"
      * Kernel 3x3
      * Stride 1x1
      * Padding SAME
      * with Output-Shape (64, 7, 7, 128)
    * Apply sequence of 4 layers with name "Conv2D_BN_ReLU":
      * Apply layer "Conv2D"
        * Kernel 3x3
        * Stride 1x1
        * Padding SAME
        * with Output-Shape (64, 7, 7, 128) without Bias
      * Batch-Normalization
      * ReLU Activation function
      * Log Featute Map in summary
    * Apply layer "Pooling"
      * Pooling-Type: MAX
      * Pooling-Window: 3
      * Stride: 2
  *** Layer: Dense_4 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * Reshape layer input (64, 4, 4, 128) to vector with 2048 elements.
        * with 1024 Output-Nodes without Bias
      * Batch-Normalization
      * ReLU Activation function
    * Dropout with keep ratio 0.5
  *** Layer: Dense_5 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * with 256 Output-Nodes without Bias
      * Batch-Normalization
      * ReLU Activation function
  *** Layer: Dense_6 ***
    * Apply sequence of 3 layers with name "Dense_BN_ReLU":
      * Apply layer "Dense"
        * with 64 Output-Nodes without Bias
      * Batch-Normalization
      * ReLU Activation function
  *** Layer: Dense_7 ***
    * Apply layer "Dense"
      * with 10 Output-Nodes
* network Output-Shape: (64, 10)
Finished to build network with 3124362 trainable variables in 38 tensors.

# Using Checkpoint:
* Checkpoint\State_0\model_120.ckpt

# Using Training-Settings:
{
  "Data": {
    "BatchSize": 64,
    "ImageHeight": 32,
    "ImageWidth": 32,
    "TrainingPath": "C:/Data/Cifar-10/training",
    "ValidatingPath": "C:/Data/Cifar-10/validation"
  },
  "Optimizer": {
    "EpochsPerDecay": 30,
    "LearnRateDecay": 0.5,
    "Momentum": 0.9,
    "StartingLearningRate": 0.003,
    "WeightDecay": 0.0
  },
  "PreProcessing": {
    "MeanFile": "image-mean.tfrecord"
  },
  "Trainer": {
    "CheckpointEpochs": 2,
    "CheckpointPath": "Checkpoint",
    "EpochSize": 50000,
    "NumberOfEpochs": 120,
    "SummaryPath": "Summary"
  },
  "Validation": {
    "Samples": 10000
  }
}

# Using Evaluation-Settings:
{
  "Data": {
    "BatchSize": 64,
    "ImageHeight": 32,
    "ImageWidth": 32,
    "ValidatingPath": "C:/Data/Cifar-10/validation"
  },
  "Evaluator": {
    "CheckpointPath": "Checkpoint",
    "EpochSize": 1000,
    "NumberOfEpochs": 10
  },
  "PreProcessing": {
    "MeanFile": "image-mean.tfrecord"
  }
}

# Evaluation Results:
Full Summary:
 *  Error: 10.34%